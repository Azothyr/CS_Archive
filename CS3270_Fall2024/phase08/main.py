import logging as logimport timeimport osfrom file_handler import FileHandlerfrom cluster_csv_handler import ClusterCSVHandlerimport pyspark.sql.functionsos.environ["PYSPARK_PYTHON"] = "C:/Users/zacst/AppData/Local/Programs/Python/Python312/python.exe"os.environ["PYSPARK_DRIVER_PYTHON"] = "C:/Users/zacst/AppData/Local/Programs/Python/Python312/python.exe"def configure_logging(level: int = log.DEBUG, exclude: list[str] = None):    """    Configure the logging module with the specified level and format.    """    if exclude is None:        exclude = []    for name in exclude:        if log.getLogger(name).hasHandlers():            log.getLogger(name).setLevel(log.CRITICAL)        log.getLogger(name).setLevel(log.CRITICAL)    log_level = level    log_format = "%(filename)s:%(lineno)d | %(asctime)s | %(levelname)s | \n\t%(message)s\n"    log_datefmt = "%Y-%m-%d (%H:%M:%S)"    log.basicConfig(        level=log_level,        format=log_format,        datefmt=log_datefmt,    )def main():    """    Main function to run the program: opens a file dialog, processes data, calculates statistics,    and visualizes weather trends.    """    # Initialize the FileHandler    file_handler = FileHandler()    try:        # Retrieve the file path using the FileHandler class        file_path = file_handler.get_file(file_type="csv")    except FileNotFoundError as e:        log.error(f"File not found: {e}")        return    except Exception as e:        log.error(f"Unexpected error: {e}")        return    start_time = time.time()    # Initialize the CSVHandler to process the CSV file    csv_handler = ClusterCSVHandler(file_path)    csv_handler.set_log_level("ERROR")    # Data analysis for numeric columns    columns = [        'humidity',  # Humidity percentage        'wind_mph',  # Wind speed in miles per hour        'wind_degree',  # Wind direction in degrees        'temperature_fahrenheit',  # Temperature in Fahrenheit        'precip_in',  # Precipitation in inches    ]    try:        data = csv_handler.get_data(columns=columns)        for col in data.columns:            summary_stats = data.select(                pyspark.sql.functions.mean(col).alias("mean"),                pyspark.sql.functions.median(col).alias("median"),                pyspark.sql.functions.stddev(col).alias("stddev"),                pyspark.sql.functions.min(col).alias("min"),                pyspark.sql.functions.max(col).alias("max")            ).collect()[0]            log.debug(                f"Summary for {col}"                f"\n\tMean: {summary_stats['mean']}"                f"\n\tMedian: {summary_stats['median']}"                f"\n\tStandard deviation: {summary_stats['stddev']}"                f"\n\tMin: {summary_stats['min']}"                f"\n\tMax: {summary_stats['max']}"            )        elapsed_time = time.time() - start_time        log.info(f"Elapsed time to completion: {elapsed_time:.4f} seconds")    except Exception as e:        log.error(f"Error processing data: {e}")        returnif __name__ == "__main__":    exclusion_list = [        "matplotlib",        "seaborn",        "pyspark",        "py4j"]    configure_logging(exclude=exclusion_list)    try:        main()    except Exception as e:        log.error(f"An error occurred: {e}")        raise